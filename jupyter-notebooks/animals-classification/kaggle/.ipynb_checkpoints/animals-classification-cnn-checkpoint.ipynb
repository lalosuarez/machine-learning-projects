{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from tensorflow import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv2D\n",
    "# from keras.layers import MaxPooling2D\n",
    "# from keras.layers import Flatten\n",
    "# from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# For image data pre processing keras has a way to differenciate the class labels\n",
    "# 1.- create test set and train set folders\n",
    "# 2.- under each folder create the necesary folders for the class labels\n",
    "# in this case: a cats folder and a dogs folder.\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Convolution\n",
    "# Apply feature detectors\n",
    "# Common practice is to start with 32 feature detectors with 3x3 dimensions\n",
    "# input_shape means the format of the image, in this case 64x64 dimension with 3 channels\n",
    "# because we are using color images, chose 2 for black and white images\n",
    "classifier.add(keras.layers.Conv2D(filters = 32, kernel_size = (3, 3), input_shape = (64, 64, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Pooling\n",
    "# pool_size 2x2 is used most of the time\n",
    "classifier.add(keras.layers.MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second convolutional layer to improve the model\n",
    "# You can also add another fully connected layer, in this case\n",
    "# by experimenting we decided to add the second convolutional layer.\n",
    "# A common practice that leads great results is to increase the feature\n",
    "# detectors by double it each time\n",
    "classifier.add(keras.layers.Conv2D(filters = 32, kernel_size = (3, 3), activation = 'relu'))\n",
    "classifier.add(keras.layers.MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Flattening\n",
    "classifier.add(keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Full connection\n",
    "classifier.add(keras.layers.Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(keras.layers.Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n",
      "63/63 [==============================] - 9s 137ms/step - loss: 0.6097 - acc: 0.6580\n",
      "250/250 [==============================] - 61s 246ms/step - loss: 0.6630 - acc: 0.5874 - val_loss: 0.6097 - val_acc: 0.6580\n",
      "Epoch 2/25\n",
      "63/63 [==============================] - 8s 122ms/step - loss: 0.5427 - acc: 0.7265\n",
      "250/250 [==============================] - 55s 220ms/step - loss: 0.5871 - acc: 0.6880 - val_loss: 0.5427 - val_acc: 0.7265\n",
      "Epoch 3/25\n",
      "63/63 [==============================] - 8s 122ms/step - loss: 0.5679 - acc: 0.7235\n",
      "250/250 [==============================] - 55s 220ms/step - loss: 0.5485 - acc: 0.7236 - val_loss: 0.5679 - val_acc: 0.7235\n",
      "Epoch 4/25\n",
      "63/63 [==============================] - 8s 124ms/step - loss: 0.5000 - acc: 0.7530\n",
      "250/250 [==============================] - 55s 220ms/step - loss: 0.5175 - acc: 0.7421 - val_loss: 0.5000 - val_acc: 0.7530\n",
      "Epoch 5/25\n",
      "63/63 [==============================] - 8s 123ms/step - loss: 0.4842 - acc: 0.7650\n",
      "250/250 [==============================] - 55s 219ms/step - loss: 0.4939 - acc: 0.7602 - val_loss: 0.4842 - val_acc: 0.7650\n",
      "Epoch 6/25\n",
      "63/63 [==============================] - 8s 122ms/step - loss: 0.4552 - acc: 0.7945\n",
      "250/250 [==============================] - 55s 220ms/step - loss: 0.4790 - acc: 0.7689 - val_loss: 0.4552 - val_acc: 0.7945\n",
      "Epoch 7/25\n",
      "63/63 [==============================] - 8s 121ms/step - loss: 0.4647 - acc: 0.7760\n",
      "250/250 [==============================] - 55s 219ms/step - loss: 0.4717 - acc: 0.7694 - val_loss: 0.4647 - val_acc: 0.7760\n",
      "Epoch 8/25\n",
      "63/63 [==============================] - 8s 122ms/step - loss: 0.4495 - acc: 0.7900\n",
      "250/250 [==============================] - 55s 219ms/step - loss: 0.4513 - acc: 0.7835 - val_loss: 0.4495 - val_acc: 0.7900\n",
      "Epoch 9/25\n",
      "63/63 [==============================] - 8s 123ms/step - loss: 0.4672 - acc: 0.7720\n",
      "250/250 [==============================] - 55s 219ms/step - loss: 0.4444 - acc: 0.7918 - val_loss: 0.4672 - val_acc: 0.7720\n",
      "Epoch 10/25\n",
      "63/63 [==============================] - 8s 122ms/step - loss: 0.4562 - acc: 0.7975\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 0.4325 - acc: 0.7955 - val_loss: 0.4562 - val_acc: 0.7975\n",
      "Epoch 11/25\n",
      "63/63 [==============================] - 8s 127ms/step - loss: 0.4480 - acc: 0.8015\n",
      "250/250 [==============================] - 55s 221ms/step - loss: 0.4256 - acc: 0.7991 - val_loss: 0.4480 - val_acc: 0.8015\n",
      "Epoch 12/25\n",
      "63/63 [==============================] - 8s 121ms/step - loss: 0.4866 - acc: 0.7785\n",
      "250/250 [==============================] - 55s 220ms/step - loss: 0.4150 - acc: 0.8060 - val_loss: 0.4866 - val_acc: 0.7785\n",
      "Epoch 13/25\n",
      "63/63 [==============================] - 8s 123ms/step - loss: 0.4166 - acc: 0.8145\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 0.4073 - acc: 0.8102 - val_loss: 0.4166 - val_acc: 0.8145\n",
      "Epoch 14/25\n",
      "63/63 [==============================] - 8s 122ms/step - loss: 0.4235 - acc: 0.8140\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 0.3883 - acc: 0.8201 - val_loss: 0.4235 - val_acc: 0.8140\n",
      "Epoch 15/25\n",
      "63/63 [==============================] - 8s 121ms/step - loss: 0.4136 - acc: 0.8170\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.3830 - acc: 0.8261 - val_loss: 0.4136 - val_acc: 0.8170\n",
      "Epoch 16/25\n",
      "63/63 [==============================] - 8s 123ms/step - loss: 0.4179 - acc: 0.8205\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 0.3785 - acc: 0.8285 - val_loss: 0.4179 - val_acc: 0.8205\n",
      "Epoch 17/25\n",
      "63/63 [==============================] - 8s 120ms/step - loss: 0.4361 - acc: 0.8055\n",
      "250/250 [==============================] - 55s 219ms/step - loss: 0.3629 - acc: 0.8326 - val_loss: 0.4361 - val_acc: 0.8055\n",
      "Epoch 18/25\n",
      "63/63 [==============================] - 8s 121ms/step - loss: 0.4174 - acc: 0.8220\n",
      "250/250 [==============================] - 54s 217ms/step - loss: 0.3612 - acc: 0.8360 - val_loss: 0.4174 - val_acc: 0.8220\n",
      "Epoch 19/25\n",
      "31/63 [=============>................] - ETA: 4s - loss: 0.4115 - acc: 0.8185"
     ]
    }
   ],
   "source": [
    "# Part 2 - Fitting the CNN to the images\n",
    "# this is to prevent over fitting by using image augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
    "                                                            shear_range = 0.2,\n",
    "                                                            zoom_range = 0.2,\n",
    "                                                            horizontal_flip = True)\n",
    "\n",
    "test_datagen = keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# target_size is the dimension expected in the cnn model, in this case 64x64\n",
    "# this applies image augmentation to the training set\n",
    "training_set = train_datagen.flow_from_directory('../input/dataset/dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "# target_size is the dimension expected in the cnn model, in this case 64x64\n",
    "# this applies image augmentation to the test set\n",
    "test_set = test_datagen.flow_from_directory('../input/dataset/dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "# steps_per_epoch means the number of images in the training set\n",
    "# validation_steps means the number of images in the test set\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 8000,\n",
    "                         epochs = 25,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making new predictions\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('../input/dataset/dataset/single_prediction/cat_or_dog_1.jpg',\n",
    "                            target_size = (64, 64))\n",
    "# Creates a 3D array because we are using a color image to have input_shape = (64, 64, 3)\n",
    "test_image = image.img_to_array(test_image)\n",
    "# Creates a 4th dimension to the image, otherwise predict method will throw an error\n",
    "# bicause it expects an input of 4 dimensions\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export model to TF SavedModel for CMLE prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Convert Keras model to TensorFlow estimator\\nmodel_input_name = classifier.input_names[0]\\nestimator_model = keras.estimator.model_to_estimator(keras_model = classifier, model_dir = \"./estimator_model/keras\")\\nprint(model_input_name)\\n!ls -lh'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Convert Keras model to TensorFlow estimator\n",
    "model_input_name = classifier.input_names[0]\n",
    "estimator_model = keras.estimator.model_to_estimator(keras_model = classifier, model_dir = \"./estimator_model/keras\")\n",
    "print(model_input_name)\n",
    "!ls -lh\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Next, export the TensorFlow Estimator to SavedModel\\nfrom functools import partial\\nimport tensorflow as tf\\n\\ndef serving_input_receiver_fn():\\n    input_ph = tf.placeholder(tf.string, shape=[None], name='image_binary')\\n    images = tf.map_fn(partial(tf.image.decode_image, channels=3), input_ph, dtype=tf.uint8)\\n    images = tf.cast(images, tf.float32) / 255.\\n    # IMPORTANT: make sure to pass the right image dimension and channel (color/black-white images)\\n    images.set_shape([None, 64, 64, 3])\\n\\n    # the first key is the name of first layer of the (keras) model. \\n    # The second key is the name of the key that will be passed in the prediction request\\n    return tf.estimator.export.ServingInputReceiver({model_input_name: images}, {'bytes': input_ph})\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Next, export the TensorFlow Estimator to SavedModel\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "\n",
    "def serving_input_receiver_fn():\n",
    "    input_ph = tf.placeholder(tf.string, shape=[None], name='image_binary')\n",
    "    images = tf.map_fn(partial(tf.image.decode_image, channels=3), input_ph, dtype=tf.uint8)\n",
    "    images = tf.cast(images, tf.float32) / 255.\n",
    "    # IMPORTANT: make sure to pass the right image dimension and channel (color/black-white images)\n",
    "    images.set_shape([None, 64, 64, 3])\n",
    "\n",
    "    # the first key is the name of first layer of the (keras) model. \n",
    "    # The second key is the name of the key that will be passed in the prediction request\n",
    "    return tf.estimator.export.ServingInputReceiver({model_input_name: images}, {'bytes': input_ph})\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"export_path = estimator_model.export_savedmodel(export_dir_base = './export',\\n                                                serving_input_receiver_fn = serving_input_receiver_fn)\\nexport_path\\n!ls -lh export/\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"export_path = estimator_model.export_savedmodel(export_dir_base = './export',\n",
    "                                                serving_input_receiver_fn = serving_input_receiver_fn)\n",
    "export_path\n",
    "!ls -lh export/\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras exports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 13M\r\n",
      "-rw-r--r-- 1 root root  14K Apr 17 02:41 __notebook__.ipynb\r\n",
      "-rw-r--r-- 1 root root  954 Apr 17 03:04 __output__.json\r\n",
      "-rw-r--r-- 1 root root 3.2M Apr 17 03:04 classifier.h5\r\n",
      "-rw-r--r-- 1 root root 2.8K Apr 17 03:04 classifier.json\r\n",
      "-rw-r--r-- 1 root root 9.4M Apr 17 03:04 full_classifier.h5\r\n"
     ]
    }
   ],
   "source": [
    "with open('classifier.json', 'w') as f:\n",
    "    f.write(classifier.to_json())\n",
    "classifier.save_weights('./classifier.h5')\n",
    "\n",
    "classifier.save('./full_classifier.h5')\n",
    "!ls -lh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
